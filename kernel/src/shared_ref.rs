//! Reference counting.
use alloc::alloc::Layout;
use alloc::alloc::alloc;
use alloc::boxed::Box;
use core::any::Any;
use core::fmt;
use core::marker::Unsize;
use core::mem;
use core::mem::offset_of;
use core::ops::CoerceUnsized;
use core::ops::Deref;
use core::ptr::NonNull;
use core::sync::atomic;
use core::sync::atomic::AtomicUsize;
use core::sync::atomic::Ordering;

use ftl_types::error::ErrorCode;

use crate::handle::Handleable;

/// The storage for a reference-counted object.
///
/// `SharedRef<T>`s store a pointer to this struct.
pub struct RefCounted<T: ?Sized> {
    counter: AtomicUsize,
    value: T,
}

impl<T> RefCounted<T> {
    const fn new(value: T) -> Self {
        Self {
            counter: AtomicUsize::new(1),
            value,
        }
    }

    pub const fn new_static(value: T) -> Self {
        Self {
            // TODO: Better way to guarantee the static reference won't be dropped.
            counter: AtomicUsize::new(usize::MAX / 2),
            value,
        }
    }
}

/// A reference-counted object.
///
/// # Why not `Arc`?
///
/// Rust's standard library provides `Arc` for reference counting. However, we
/// generally prefer rolling our own primitives in kernel to use what we really
/// need:
///
/// - Allocation errors must be handled by the caller, not panicking.
///
/// - Some objects are statically allocated, and can be initialized at compile
///   time. We want to avoid Lazy<SharedRef<T>>, which has an overhead of
///   the check for emptiness.
///
/// - We'll never need weak references. Instead, the userland will delete each
///   object explicitly through a system call (LMK if you noticed counter-examples!).
///
/// # Atomic Operations on counters
///
/// [`Ordering`] parameters are chosen to be as relaxed as possible in the fast
/// path, inspired by Rust's `Arc` implementation.
pub struct SharedRef<T: ?Sized> {
    ptr: NonNull<RefCounted<T>>,
}

impl<T> SharedRef<T> {
    /// Creates a new reference-counted object.
    pub fn new(value: T) -> Result<Self, ErrorCode> {
        let layout = Layout::new::<RefCounted<T>>();
        let ptr = unsafe { alloc(layout) as *mut RefCounted<T> };
        if ptr.is_null() {
            return Err(ErrorCode::OutOfMemory);
        }

        unsafe {
            ptr.write(RefCounted::new(value));
        }

        Ok(Self {
            ptr: unsafe { NonNull::new_unchecked(ptr) },
        })
    }

    /// Creates a new reference-counted object from a static reference.
    pub const fn new_static(inner: &'static RefCounted<T>) -> Self {
        let ptr = inner as *const RefCounted<T> as *mut RefCounted<T>;
        Self {
            ptr: unsafe { NonNull::new_unchecked(ptr) },
        }
    }

    /// Provides a raw pointer to the data.
    ///
    /// The counts are not affected in any way and the SharedRef is not consumed.
    /// The pointer is valid for as long as there are strong counts in the SharedRef.
    pub fn as_ptr(&self) -> *const T {
        &self.inner().value as *const T
    }

    /// Consumes the SharedRef, returning the wrapped pointer.
    ///
    /// To avoid a memory leak the pointer must be converted back to a SharedRef
    /// using `SharedRef::from_raw`.
    pub fn into_raw(self) -> *const T {
        let ptr = self.as_ptr();
        core::mem::forget(self);
        ptr
    }

    /// Constructs a SharedRef from a raw pointer. The reference count
    /// is not incremented.
    ///
    /// # Safety
    ///
    /// The raw pointer must have been previously returned by a call to
    /// `SharedRef::into_raw`.
    pub unsafe fn from_raw(ptr: *const T) -> Self {
        unsafe {
            // Calculate the pointer to RefCounted<T> from the pointer to T
            let ref_counted =
                (ptr as *const u8).sub(offset_of!(RefCounted<T>, value)) as *mut RefCounted<T>;
            Self {
                ptr: NonNull::new_unchecked(ref_counted),
            }
        }
    }
}

impl<T: ?Sized> SharedRef<T> {
    /// Creates a new reference-counted object from a static reference.
    pub const fn clone_static(this: &'static SharedRef<T>) -> Self {
        // Static references are guaranteed to be alive for the lifetime of the
        // program. Create the new SharedRef without incrementing the reference
        // count.
        Self { ptr: this.ptr }
    }

    /// Returns true if the two pointers point to the same object.
    pub fn ptr_eq(a: &Self, b: &Self) -> bool {
        core::ptr::addr_eq(a.ptr.as_ptr(), b.ptr.as_ptr())
    }

    /// Returns a reference to the inner object.
    fn inner(&self) -> &RefCounted<T> {
        // SAFETY: The object will be kept alive as long as `self` is alive.
        //         The compiler will guarantee `&RefCounted<T>` can't outlive
        //         `self`.
        unsafe { self.ptr.as_ref() }
    }
}

impl<T: ?Sized> Drop for SharedRef<T> {
    fn drop(&mut self) {
        debug_assert!(self.inner().counter.load(Ordering::Relaxed) > 0);

        // Release the reference count.
        if self.inner().counter.fetch_sub(1, Ordering::Release) == 1 {
            // The reference counter reached zero. Free the memory.

            // "Prevent reordering of use of the data and deletion of the data",
            // as the standard library's `Arc` does [1].
            //
            // [1]: https://github.com/rust-lang/rust/blob/da159eb331b27df528185c616b394bb0e1d2a4bd/library/alloc/src/sync.rs#L2469-L2497
            atomic::fence(Ordering::Acquire);

            // SAFETY: This reference was the last one, so we can safely
            //         free the memory.
            mem::drop(unsafe { Box::from_raw(self.ptr.as_ptr()) });
        }
    }
}

impl<T: ?Sized> Clone for SharedRef<T> {
    fn clone(&self) -> Self {
        debug_assert!(self.inner().counter.load(Ordering::Relaxed) > 0);

        // Increment the reference count.
        //
        // Theoretically, the counter can overflow, but it's not a problem
        // in practice because having 2^B references (where B is 32 or 64
        // depending on the CPU) means you have at least 2^B * size_of(NonNull)
        // bytes of space. Who would have that much memory to store references
        // to a *single* object?
        self.inner().counter.fetch_add(1, Ordering::Relaxed);

        Self { ptr: self.ptr }
    }
}

impl<T: ?Sized> Deref for SharedRef<T> {
    type Target = T;

    fn deref(&self) -> &Self::Target {
        &self.inner().value
    }
}

impl<T> fmt::Debug for SharedRef<T>
where
    T: fmt::Debug,
{
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        f.debug_tuple("SharedRef")
            .field(&self.inner().value)
            .finish()
    }
}

impl SharedRef<dyn Handleable> {
    pub fn downcast<T>(self) -> Result<SharedRef<T>, Self>
    where
        T: Handleable,
    {
        if <dyn Any>::is::<T>(&self.inner().value) {
            let ptr = self.ptr.cast();
            mem::forget(self);
            Ok(SharedRef { ptr })
        } else {
            Err(self)
        }
    }
}

unsafe impl<T: Sync + Send + ?Sized> Sync for SharedRef<T> {}
unsafe impl<T: Sync + Send + ?Sized> Send for SharedRef<T> {}

impl<T: ?Sized + Unsize<U>, U: ?Sized> CoerceUnsized<SharedRef<U>> for SharedRef<T> {}
